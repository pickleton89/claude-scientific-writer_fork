# Literature Review - Expert Guide

> Principles, decision rules, and failure modes that separate competent from excellent execution.
> Generated via expert synthesis prompt.

---

## 1. Core Principles

1. **Synthesis over Summary**: Expert reviews analyze relationships between studies—patterns, contradictions, consensus—rather than listing study-by-study descriptions. The reader should understand the *field*, not individual papers.

2. **Reproducible Search Strategy**: Every search decision must be documented with enough precision that another researcher could replicate it exactly. Databases, dates, search strings, inclusion criteria, exclusion counts—all recorded.

3. **Thematic Architecture**: Organize by concepts and themes that answer your research question, not by chronology or author. Studies appear where their contribution matters, potentially in multiple sections.

4. **Critical Stance, Not Neutral Summary**: Evaluate evidence quality, identify methodological limitations, and weight findings accordingly. A review that treats all sources equally has failed at its core function.

5. **Gap Identification as Primary Output**: The most valuable contribution is articulating what remains unknown, understudied, or methodologically flawed—this shapes future research.

## 2. Decision Framework

| Situation | Action |
|-----------|--------|
| Research question is broad | Narrow with PICO/SPIDER framework before searching |
| Single database yields sufficient results | Search minimum 3-4 databases anyway; single-source reviews miss 30-50% of relevant literature |
| High-quality and low-quality studies on same topic | Weight findings by study quality; explicitly acknowledge evidence strength differences |
| Studies contradict each other | Investigate methodological differences that explain discrepancy; this is often the most valuable finding |
| Search returns >500 results | Refine search terms rather than screen all; pilot searches should yield 50-200 manageable hits |
| Preprint exists for peer-reviewed paper | Cite the peer-reviewed version; note if significant changes occurred between versions |
| Key seminal paper is >10 years old | Include it but verify findings with recent replication studies; note if assumptions have been challenged |

## 3. Concrete Patterns

**Multi-Database Triangulation**
- [Need comprehensive coverage] → [Search PubMed + Scopus/Web of Science + domain-specific database + preprint server] → [Each database has different indexing; overlap is typically only 60-70%]

**Evidence Pyramid Weighting**
- [Multiple study types on same question] → [Prioritize systematic reviews > RCTs > cohort > case-control > case series > expert opinion] → [Higher-level evidence inherently controls for more confounders]

**Citation Network Expansion**
- [Initial search complete but potentially incomplete] → [Check references of included papers (backward) + papers citing key works (forward via Semantic Scholar/Google Scholar)] → [Captures papers with different terminology or missed by keyword search]

## 4. Failure Modes

| Mistake | Why It Fails |
|---------|--------------|
| Starting writing before search is complete | Creates confirmation bias; you find sources that fit your narrative rather than letting evidence shape conclusions |
| Organizing by author/study | Produces a disconnected list; readers can't extract the synthesized understanding that is the review's purpose |
| Including every source found | Dilutes signal with noise; irrelevant or low-quality sources obscure key findings |
| Accepting abstracts as accurate representations | Abstracts often overstate findings or omit limitations; full-text review catches 15-20% mischaracterizations |
| Treating all databases as equivalent | Specialized databases (CINAHL for nursing, PsycINFO for psychology) capture domain literature that generalist databases miss |
| Finishing without citation verification | 5-10% of citations contain errors in author names, titles, or DOIs that undermine credibility |

## 5. Anti-Patterns

- **Never** write the literature review before completing the systematic search—narrative-first approaches embed confirmation bias
- **Never** rely on Google Scholar alone—it lacks advanced filtering and reproducible search strings
- **Never** exclude preprints without justification—in fast-moving fields, 12-24 months of relevant research exists only in preprint form
- **Never** use "et al." in the reference list—always list all authors for verification and credit
- **Never** include a source you haven't read in full—abstracts and secondary citations are insufficient

## 6. Quality Signals

| Good Enough | Excellent |
|-------------|-----------|
| Lists inclusion/exclusion criteria | Provides rationale for each criterion tied to research question |
| Reports database search results | Includes complete search strings reproducible by others |
| Groups studies by topic | Integrates studies into narrative that reveals field structure and debates |
| Notes study limitations | Weights conclusions based on quality assessment using validated tools (RoB2, NOS, AMSTAR 2) |
| Identifies "gaps" generically | Specifies precise research questions that would fill gaps, including suggested methodologies |
| PRISMA diagram present | PRISMA diagram includes exclusion reasons at each stage with counts |

---

## Final Synthesis

**Do This:**
- Register your protocol before starting (PROSPERO, OSF)
- Use minimum 3-4 databases plus citation chaining
- Document search strategy in executable detail
- Assess study quality with validated tools
- Synthesize thematically, not study-by-study

**Not That:**
- Starting searches without defined inclusion criteria
- Relying on single database or Google Scholar alone
- Accepting findings without critical quality assessment
- Organizing as annotated bibliography of summaries
- Treating all evidence as equally valid

**When In Doubt:** Ask "Could another researcher reproduce my exact search and screening decisions from this documentation?"

**Quality Check:**
1. Can I regenerate my search results using only my documented search strategy?
2. Are my inclusion/exclusion criteria specific enough to yield consistent decisions?
3. Does every paragraph in my results section synthesize across multiple sources?
4. Have I assessed study quality using appropriate validated tools?
5. Do my conclusions appropriately weight evidence by quality?
6. Have I verified every DOI and citation for accuracy?
