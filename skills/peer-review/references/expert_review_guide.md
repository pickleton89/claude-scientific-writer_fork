# Expert Peer Review Guide

> Principles, decision rules, and failure modes that separate competent from excellent peer review.

---

## Core Principles

1. **Evaluate the science, not the scientists.** Assess whether claims are supported by evidence, not whether authors are prestigious or topics fashionable.

2. **Proportionality of scrutiny.** Claim strength must match evidence strength. Extraordinary claims demand extraordinary rigor; incremental contributions need appropriate—not excessive—validation.

3. **Falsifiability over confirmation.** Actively search for ways conclusions could be wrong. Steelman the paper first, then probe for weaknesses.

4. **Actionability over judgment.** A review that says "this is flawed" without "here's how to fix it" wastes everyone's time.

5. **Hierarchical feedback.** Not all issues are equal. Distinguish fatal flaws from fixable problems from polish.

---

## Decision Framework

| When... | Then... |
|---------|---------|
| Methods are underspecified | Ask: "Could I replicate this?" If no → Major issue |
| Statistics look suspicious | Check: assumptions met? Multiple testing corrected? Effect sizes reported? |
| Conclusions feel overstated | Map claims back to specific data. Unsupported → flag it |
| Paper has many minor issues | Categorize ruthlessly. Don't bury major concerns in typos |
| You're uncertain about a subfield | State your limitation. Don't bluff expertise |
| Authors cite themselves excessively | Check if citations are substantive or self-promotional |

### Key Trade-offs

- **Thoroughness vs. proportionality** — Don't nitpick a methods paper on prose
- **Rigor vs. scope creep** — Don't demand experiments beyond the paper's claims
- **Constructive vs. permissive** — Be helpful, but don't lower standards

---

## Expert Patterns

### Pattern 1: The Claim-Evidence Audit

**Situation:** Author makes strong mechanistic claim
**Action:** Trace backward—which figure supports this? What control rules out alternatives?
**Rationale:** Overstated conclusions are the #1 failure mode. Direct mapping exposes gaps.

### Pattern 2: The Replication Test

**Situation:** Methods section looks complete
**Action:** Mentally walk through executing the protocol. Note every missing parameter.
**Rationale:** Authors know their methods unconsciously. Outsiders reveal implicit assumptions.

### Pattern 3: The Inverse Review

**Situation:** Paper seems solid but something feels off
**Action:** Ask: "What would the authors need to show to convince a skeptic?" Check if they showed it.
**Rationale:** Shifts from passive reading to active hypothesis testing.

---

## Failure Modes

| Mistake | Why It Fails |
|---------|--------------|
| **Reviewing the paper you wish they'd written** | Demanding out-of-scope experiments; confusing your agenda with scientific necessity |
| **Death by minor comments** | 47 small issues obscure 2 critical ones; authors fix typos while ignoring fatal flaws |
| **Confirmation bias** | Accepting work that confirms your priors; scrutinizing work that challenges them |
| **The expertise bluff** | Reviewing statistics/methods you don't understand rather than flagging uncertainty |
| **Tone deafness** | Harsh criticism demotivates and obscures your points; unhelpful even when correct |
| **Treating all papers identically** | A Nature paper and a methods paper require different evaluation criteria |

---

## Anti-Patterns

**Never do these, even if they seem efficient or standard:**

- Request experiments not necessary for current claims (scope creep disguised as rigor)
- Use the review to advance your own citations
- Make it personal ("The authors clearly don't understand...")
- Give vague criticism without specifics ("The statistics are problematic")
- Recommend rejection on impact/novelty alone (that's editorial, not validity)

---

## Quality Signals

| Good Enough | Excellent |
|-------------|-----------|
| Identifies major issues | Identifies issues AND explains why AND suggests fixes |
| Summary matches comments | Summary synthesizes, doesn't just list |
| Proportional coverage | Calibrated scrutiny to claims that matter most |
| Professional tone | Constructive tone authors will learn from |
| Checks the boxes | Demonstrates genuine engagement with the science |

### Subtle Expert Markers

- Comments reference specific figures/tables/line numbers
- Statistical concerns cite specific tests and assumptions
- Suggestions are concrete ("try Mann-Whitney") not vague ("use appropriate statistics")
- Acknowledges what works before identifying what doesn't
- Distinguishes "I would do it differently" from "this is wrong"

---

## Quick Reference

### Do This

- Read twice: first for the story, second for the evidence
- Structure hierarchically: summary → major → minor → optional
- Be specific: page, paragraph, figure, line
- Suggest solutions, not just problems
- Calibrate rigor to claims

### Not That

- Don't bury critical issues in minor comments
- Don't demand experiments outside scope
- Don't review the paper you wish they'd written
- Don't give vague criticism
- Don't let 50 minor issues obscure 3 major ones

### When In Doubt

> "If I were the author, would this feedback help me make the paper better?"
> If not, rewrite it.

---

## Self-Review Checklist

Before submitting your review:

1. Does my summary accurately reflect my detailed comments?
2. Have I distinguished fatal flaws from fixable problems from preferences?
3. Are my major comments actually major (would block publication if unaddressed)?
4. Did I provide specific, actionable suggestions?
5. Would a reasonable author find this feedback fair and useful?
6. Have I flagged areas where I lack expertise to judge?
