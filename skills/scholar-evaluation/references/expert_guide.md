# Scholar Evaluation — Expert Guide

> Principles, decision rules, and failure modes that separate competent from excellent research evaluation.
> Generated via expert synthesis prompt.

---

## 1. Core Principles

1. **Evaluate claims against evidence, not reputation.** The quality of an argument is independent of the author's institution, journal prestige, or citation count. Judge the work on its internal logic and empirical support.

2. **Proportionality of scrutiny.** Match evaluation rigor to the strength of claims. Bold claims require stronger evidence chains; incremental contributions need appropriate—not excessive—verification.

3. **Separate dimensions, then synthesize.** Methodology, novelty, clarity, and significance are orthogonal. A paper can have excellent methods but trivial contribution, or novel ideas with weak execution. Conflating dimensions produces incoherent assessments.

4. **Synthesis over summarization.** Expert evaluation identifies patterns across dimensions and reveals what the work means in context. Merely listing strengths and weaknesses is a checklist, not an assessment.

5. **Actionability is mandatory.** Every identified weakness must have a path to resolution. Criticism without direction wastes everyone's time.

---

## 2. Decision Framework

| When... | Then... |
|---------|---------|
| Claims exceed evidence scope | Flag specifically which claims lack support and what evidence would be needed |
| Methods are underspecified | Ask: "Could I replicate this with only what's provided?" If no → Major issue |
| Novel contribution is unclear | Demand explicit statement of delta from prior work |
| Multiple weaknesses exist | Rank by impact on validity, not by order encountered |
| Work is interdisciplinary | Acknowledge dimension expertise gaps; don't bluff authority |
| Statistical reporting is incomplete | Check: effect sizes, confidence intervals, multiple testing correction |
| Reviewer and author disagree on scope | Evaluate what the paper claims to do, not what you wish it did |

**Key trade-offs:**
- **Rigor vs. scope creep**: Demand evidence for stated claims only, not imagined extensions
- **Depth vs. coverage**: Better to deeply evaluate 5 dimensions than superficially cover 10
- **Standards vs. context**: Adjust expectations for stage (draft vs. submission) and venue (workshop vs. flagship)

---

## 3. Concrete Patterns

**Pattern 1: The Claim Trace**
- [Situation] → Author states "X causes Y"
- [Action] → Trace backward: Which figure/table supports this? What alternative explanations are ruled out? What's the effect size?
- [Rationale] → Overstated conclusions are the most common and consequential flaw. Direct mapping exposes gaps between data and claims.

**Pattern 2: The Replication Audit**
- [Situation] → Methods section appears complete
- [Action] → Mentally execute the protocol step-by-step. Note every parameter, threshold, or decision that's missing.
- [Rationale] → Authors internalize their methods unconsciously. Outsider perspective reveals implicit assumptions and hidden degrees of freedom.

**Pattern 3: The Contribution Delta**
- [Situation] → Work claims novelty
- [Action] → Ask: "What specific capability exists after this work that didn't exist before?" Reject vague answers like "better understanding."
- [Rationale] → Forces concrete articulation of contribution, distinguishing genuine advances from incremental variations.

---

## 4. Failure Modes

| Mistake | Why It Fails |
|---------|--------------|
| **Single-metric obsession** | Using only citations, p-values, or journal tier ignores orthogonal quality dimensions |
| **Confirmation bias** | Scrutinizing work that challenges your views while accepting confirming work uncritically |
| **Expertise bluffing** | Evaluating statistics or methods you don't understand rather than flagging uncertainty |
| **Checklist-as-evaluation** | Marking dimensions complete without synthesizing what they mean together |
| **Scope expansion** | Demanding experiments beyond the paper's stated claims |
| **Halo effects** | Letting strong methodology excuse weak contribution, or vice versa |
| **Vague criticism** | "Methods are problematic" without specifying which, why, and how to fix |

---

## 5. Anti-Patterns

**Never do these, even if they seem standard:**

- Evaluate what you wish the paper did instead of what it claims to do
- Use reputation (author, institution, journal) as a quality proxy
- Conflate "I would do it differently" with "this is wrong"
- Provide criticism without actionable remediation paths
- Apply single-discipline standards to interdisciplinary work
- Demand methodological perfection while ignoring conceptual contribution (or vice versa)
- Rate based on importance to *your* research agenda rather than the field

---

## 6. Quality Signals

| Good Enough | Excellent |
|-------------|-----------|
| Identifies major weaknesses | Identifies weaknesses AND explains impact AND suggests fixes |
| Covers all requested dimensions | Synthesizes across dimensions to reveal emergent patterns |
| Provides a recommendation | Grounds recommendation in explicit criteria with evidence |
| Notes methodology gaps | Traces gaps to specific claims they undermine |
| Lists strengths and weaknesses | Prioritizes by impact on validity and contribution |
| Uses the rubric | Adapts rubric to work context while maintaining rigor |

**Subtle expert markers:**
- Distinguishes flaws that invalidate conclusions from flaws that limit generalizability
- Identifies what's *not* in the paper that should be (missing comparisons, unaddressed alternatives)
- Calibrates expectations to venue, stage, and discipline norms
- Provides evaluation rationale, not just scores
- Acknowledges when a weakness might reflect reasonable trade-offs

---

## Final Synthesis

**Do This:**
- Trace every major claim to its supporting evidence
- Separate assessment by dimension before synthesizing
- Rank issues by impact on validity, not by order found
- Provide specific, actionable remediation for each weakness
- Calibrate rigor to claims: extraordinary claims need extraordinary evidence

**Not That:**
- Don't conflate methodology quality with contribution significance
- Don't use reputation or metrics as quality proxies
- Don't demand experiments beyond stated scope
- Don't give vague criticism ("needs improvement")
- Don't evaluate the paper you wish they'd written

**When In Doubt:**
Ask: "If I fixed everything I've flagged, would the core contribution still stand?" This separates fatal flaws from improvable weaknesses.

**Quality Check:**
1. Have I evaluated claims against their stated evidence, not my expectations?
2. Did I separate dimensions before synthesizing, or conflate them?
3. Are my criticisms actionable with specific remediation paths?
4. Have I calibrated rigor appropriately for the venue and stage?
5. Did I flag where my expertise limits my assessment authority?
6. Would a reasonable author find this evaluation fair and useful?
